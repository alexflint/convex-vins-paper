
\section{Related work}

There is a long history within the robotics community of applying the EKF to navigation problems. Early work by Smith and Cheeseman \cite{smith1986} described an approach that maintains a joint covariance over the robot position and the coordinates of surrounding landmarks. Davison \etal \cite{davison2003} described a SLAM filter based on the EKF that uses only a monocular camera. The variable--state dimension filter described by McLauchlan \cite{mclauchlan1999} provided a framework for changing the size of the state space over time. Jones and Soatto \cite{jones2011} describe an extended SLAM filter incorporating visual and inertial measurements.

A second approach to filtering is to leave the landmarks out of the filter state, which can significantly reduce computation time since the EKF is cubic in the size of the filter state, and landmarks typically account for many state variables. Soatto \etal \cite{soatto1996} used the epipolar geometry in conjunction with inertial constraints to formulate such a filter. Garcia \etal \cite{garcia2002} and Mourikis and Roumeliotis \cite{mourikis2007} described sliding window filters in which the state space consists of the device state at multiple points in time. This permits optimal visual updates without explicitly tracking a covariance over landmarks.

In contrast, the vision community has explored approaches in which navigation is cast as an optimization problem. The visual odometry algorithm of Nister \etal \cite{nister2004} solves a sequence of two-- and three--frame optimization problems using a pre--emptive RANSAC algorithm. Sibley \cite{sibley2006} described a sliding window bundle adjuster in which navigation is solved as a sequence of non--linear least squares problems. Klein and Murray \cite{klein2007} perform global bundle adjustment on a small number of frames sampled from the video stream. Strasdat \etal \cite{strasdat2011} describe a windowing approach in which spatially distant frames are included in the optimization but not actually adjusted in the update step.

All of the above approaches use a discrete--time representation in which trajectories are parameterized by the position and orientation of the device at the time video frames were captured. Recently, Furgale \etal \cite{furgale2012} presented a continuous--time formulation in which the trajectory of the device is parametrized via the control points of a spatial B--spline. Lovegrove \etal extended this approach to incorporate unknown landmarks in the optimization. A key advantage of the continuous--time approach is that it removes the need to solve an ODE when incorporating inertial measurements; instead, both the vision and inertial measurements can be incorporated into a straightforward generative model. This is crucial to the work presented in this paper since solutions of the relevant ODEs are not analytic and so are ill--suited to the convex optimization algorithms we use.

Second--order cone programming has a deep history in the convex optimization literature; an overview is given in \cite{boyd2008}. Within the computer vision literature, Kahl \cite{kahl2005} formulated and solved a number of classic reconstruction problems including triangulation, resectioning, and homography estimation as SOCP problems. A related quasi--convex optimization scheme was described by Ke \etal \cite{ke2007}. This paper builds on these ideas to formulate visual inertial navigation as a SOCP problem, and we show how to estimate not just discrete camera poses but full device trajectories, incorporating constraints based on both visual and inertial measurements.
